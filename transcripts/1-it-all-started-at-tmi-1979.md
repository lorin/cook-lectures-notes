# 1. It all started at TMI, 1979

<https://youtu.be/H-VTafOPLR4>

Feb. 3, 2014


## Introductory

Welcome everyone to the first seminar in the cognitive work studies series,
of which we expect to have seven or eight.

We begin today by congratulating Professor Miriam (?) for her promotion to
Associate Professor at (?) where she is now a person who can be in charge of
everything having to do with students and supervising PhD people and so
forth. And that means that they will only double your workload. And welcome
to all of you. We have a group of six people here in the room and we have two
people joining us by Skype. May get a third one at some point, if we get a
message from carolina tells them I don't have anything from her right now.

And she's tried to contact me yet. We're videotaping this so that people who
don't manage to get connected, we'll be able to have a copy of it later. And
we'll try and put that up as a YouTube video for you to have access to later.
If you want to be able to go back and look at something, you have to have
some way of doing that.

This is about an hour and a half total. We get done around 5:30, but in
fact, the goal is to be done with the talking about the main portion of the
seminar around five, and then to spend half an hour talking about what the
implications might be.

## Readings

The handouts for this seminar were some materials on Three Mile Island,
including a pointer to a Wikipedia article, which isn't bad. And also a
portion of what's come to be known as the Rogovin report, which is a report
that was commissioned related to the event and contains a whole bunch of
technical details as well as a nice narrative. And that was the bulk of what
we handed out.

And then there were two commentaries. The first commentary was from Thomas
Pigford, who was a nuclear engineer and somebody who is a big proponent of
nuclear safety, but also an engineering type as engineering kind of view. And
one from Bruce Babbitt, who was a former governor in the United States, who
was also on the commission that looked at this.

And then there were a couple of papers. One was [coping with complexity](https://orbit.dtu.dk/en/publications/coping-with-complexity),
which was an old early report from RISO that covered some topics that were
then of interest written by Jens rasmussen and Morten Lind. And Jens
Rasmussen's paper [skills, rules, and knowledge](https://ieeexplore.ieee.org/abstract/document/6313160).

I'm sorry that I sent out a copy of it that wasn't easily printed. It had, I
sent out a copy of that, had a print thing on it. But if you haven't got the
copy and if you haven't got a hard copy. Yeah. I have one here for you. We
can send one out. Do you have a hard copy?

## Why is Three Mile Island so important?

And most of this is a way of getting us into the topic of why Three Mile Island is so important and why it shapes all of the kinds of questions and issues that appear now, when people talk about studying cognitive work, if you try and look today at cognitive work studies, particularly those surrounding safety or human performance kinds of issues. This includes studies of people in simulators and work studies and incident reporting studies and accident investigation studies that entire collection of materials, the way it's done the way it is interpreted really draws its source from Three Mile Island.

Although human factors began earlier... really began in the late eighten hundreds with work on railroads and trying to understand safety on railroads. It took off as a kind of special discipline around the time of world war II, as people were trying to make weapons systems more effective and trying to figure out how to make guns and bombs and other things that were better and more easily controlled, and also trying to prevent plane crashes and aircraft that were happening about the time.

It was a joke in human factors that, until 1979, the only two kinds of subjects that got any attention from human factors people were when someone died who shouldn't have, or someone didn't die who should have. And that was mostly either safety of aircraft or transportation systems or the failure of weapons systems to perform the way that they wanted. And it's probably mostly true.

Most of these systems that were getting human factors attention, were getting attention because of the knobs and dials features. You all know the kind of classic human factor story about trying to shape the landing gear control so that it was physically different than the flaps control, because both were on the side of the pilot in below,and he could reach down and easily think that he was lowering the landing gear when in fact he was lowering the flaps. And by changing the landing gear handle to have the shape of a wheel so it felt like the landing gear, you provided a communication to the pilot about what control he was actually touching so that when he moved it, he knew what it was he was actually affecting. That's the sort of classic human factors thing. If you're asking for a model, what is human factors? Most people would say, ah, that's the example we would look to.

What happened over the next 20 years or so, is that people started to build very large and very complex systems that involved extraordinary amounts of energy. This was, there were power plant failures and problems with power plants, particularly steam engines blowing up, both moving steam engines and stationary ones. And that happened from the 18 early 18 hundreds all the way through the 19 hundreds and the biggest kind of industrial accident that you could have short of a bridge collapsing was to have your steam boiler explode.

And this was not an uncommon event. There are lots and lots of spectacular wrecks that occurred and buildings that were damaged and so forth. But by the 1930s or so, the control of these things was pretty well understood. Their mechanics and physics were well known and steam boiler explosions became relatively infrequent.

There were problems that occurred with other big mechanical failures, but they were purely mechanical failures, a bridge falling down, or a tunnel collapsing or things like that. That didn't seem to be very much involved with human operations at the time. So the operator related accidents that happened before three mile Island were scattered and relatively straightforward, some were caused by overt problems with human factors, but those were pretty straightforward and easy to understand.

There was a lot of discussion philosophically about what accidents meant and how they happened. This goes back all the way to 1900 or before. And people were talking about causes versus reasons and other kinds of theoretical ideas like this, but that was mostly in the form of philosophy. And wasn't really attached to any kind of empirical study.

What happened at Three Mile Island was an accident that had actually happened before, which is one of the reasons why it was so distressing to people. Three Mile Island is actually an Island in Pennsylvania, in which there were two plants that were located together . On one side was a plant that had been up and running for a while called &quot;Three Mile Island, Unit One&quot;. And then there was a plant that had a power generator that had just started up about a few months earlier called &quot;Three Mile Island, Number Two&quot;. At the time of the accident, &quot;Three Mile Island, Number One&quot; was not running, it was down for refueling. And so the only plant that was really active was this &quot;Three Mile Island, Number Two&quot;.

And as you can tell from the Wikipedia story, what happened in &quot;Three Mile Island, Number Two&quot; was this long series of events that didn't seem to be causally related in the sense that one event didn't trigger another event directly, but they were functionally related because various systems in the power plant activated in response to these failures and generated the difficulty.

What was happening was, in the non-nuclear portion of the plant, there was a device which was supposed to essentially purify the water that... I'm sorry, let me draw this differently.... there's a device, it was supposed to purify the water that came from the turbines after the steam had been used, the heat energy and the steam had been used, but came from the turbines that purified the water.

When they had some problems with this device, they were trying to fix it in the way that they normally fix things. And they did some stuff here that caused essentially a transient to occur in this that led to an emergency shutdown of the generating turbine. The generating turbine automatically tripped; this is the power generating thing that makes electricity.

This failed, because, or tripped... was actually like tripping a circuit breaker. When the condenser filtered process went down and it shut the turbine off. With the turbine off, there was no way to take energy out of the system and the nuclear reactor, which was over here was moving hot fluid, hot steam through a steam generator and the, in the steam generator, there was, there were other related pipes that were taking that hot energy. Got a little steam generator here. We're taking the energy inside the steam generator. The steam from the nuclear power plant was passing around an exchanger, which was converting energy into steam in this other circuit. So there are two complete separate circuits and the two circuits are kept separate mechanically.

And the reason is that this circuit here with the nuclear power plant in it is with the nuclear reactor. It is inside the containment vessel and therefore protected from various kinds of leakages out to the outside. The only place where anything that could possibly become radioactive comes in contact with that is in this steam generator. And the steam generator is usually flowing cold water in, and it's being heated up by the hot steam from the from the nuclear power reactor.

And that hot steam is converting the cold water from the generator side into steam, which then drives the turbine. So if you think about it, and in terms of energy, there's energy being created here, or being released here in the form of lots and lots of heat, and that heat is being transferred here out to the heat that is carried off and then used by the turbine.

And that process is stable. The plant was running at 97% of its full rate of power. And so long as this loop keeps up, the heat generated in the reactor, ends up generating electricity, which is the load. And everything's stable. When the reactor, when the turbine stops working, then no heat is being taken up in this system.

And the heat temperature inside this portion of it quickly rises because of the heat is still being generated the immediately after the turbine tripped and the turbine stopped running. Within seconds, the reactor itself tripped. And what that means is that the reactor automatically shoved the control rods that dampened the nuclear reaction into the reactor itself, turning off the chain reaction.

So there was within six or seven seconds after the turbine strip, the reactor trip, the control rods were dropped into the reactor and the nuclear reaction, the chain reaction, the generator stopped. Completely. What's interesting and about it though, is that this is a really big thing.

It's tons and tons of material and it's generating, it has tremendous amount of heat contained in it that's being carried away. The other thing is that there are lots and lots of nuclear byproducts, radioactive elements that are generated inside a nuclear reactor during the time.

Okay. Yeah. Sandy, you should be with us now. Yeah. Can you hear us?

Can you hear us? I can't tell. It looks like you're still connected from our end.

If you're not let me know. So what happened here is that the the energy continued to be high in here. There are a lot of radioactive products that are generated by the intense nuclear reactivity that's happening in the power plant. And these are many of these have very short half lives. They will decay within seconds or within minutes.

But that the K continues to produce heat. So even though you turn off the reactor, it's still a huge heat source and it continues to have heat that's needed that needs to be taken care of for a long time. Afterwards, you may remember that the Fukushima accident became a real problem. Not because the reactors were still running, but because they were still generating heat that needed to be dealt with.

And once the reactor pumps had shut down, they had no way to deal with that. Very similar problem is happening here. Inside a three mile Island though, inside of the nuclear power plant, there, there are a whole series of systems that are designed to deal with this. And one of the systems is. And automated a collection of automated systems that will dump water into the nuclear reactor to keep it cool and keep it covered with water.

And many of these operate automatically. They're so important that they've been built automatically sense that there is some problem and automatically start flowing water in. There are several of these systems. There's probably three or four distinct systems that work there. They all work in parallel.

They all work at different times. But it's not like any that that they hadn't planned for this kind of event had happened in the past. And the safety systems are intended to keep the core covered with water and cold water going through. So that in fact, the core is kept below a certain temperature.

Sure. The reason that it needs to be below that temperature is because a nuclear reactor has in it, a series of. Assemblies that contain the nuclear fuel, these fuels in the shape of pellets. They're about this big and the pellets themselves go into inside of a little kind of can that can or cylinder is made out of something that is a combination of zirconium and.

Some niobium as well, but it's called the basic thing here is called circle.

So when the root, the fuel is kept together by this thin little external can, the can itself is made out of this metal, which is designed not to keep the reaction from going on it doesn't block neutron so that the chain reaction could continue. And there's a whole bunch of these things. The problem is of course, that if you get the temperature too high in here, if you don't have water continuously flowing past these, the temperature will start to go up and it can reach a point at which the Zirkle itself will melt.

If that happens. The fuel pellet becomes exposed and can break down. And then you have in this circuit, not just very hot water with some low amount of radioactivity, but now you're starting to circulate a radioactive fuel around that cycle. These things are pretty good too for high heat, but you can't let their temperature rise too high.

So it's a critical importance that the reactor core, which is this chunk in here, that's about 15 feet tall and made out of whole bunches of tubes. This are collected in assemblies, continue to stay covered with water, the real critical component of all this. As you've got to keep that reactor covered with water.

If the reactor core is covered with water, you're doing okay. If it becomes uncovered, you're in a really bad situation. There is it a variety of systems in here that are designed to help keep the system under control. And one of them is a kind of a pressurizer and there's a valve associated with this pressurizer called the pressure, pressure pressurizer. Over pressure relief valve or PRV. And the function of the PRV is to allow pressure sure. In the reactor to exceed a certain point and then open automatically so that no damage happens to all the piping in here. That pressure is in the thousand PSI range. And the por V does this automatically.

It doesn't, it's not, it doesn't have to be told to do this. It will do it beyond a certain. Certain level, but it can also be opened and closed by a solo night. The por V in this reactor had been sticking open. There were problems with it. There were known problems with PRV valves and other reactors of this design.

One of the problems with the PRV is that if it opens up and lets the pressure fall, the coolant and the reactor begins to boil, you have a . A pressure cooker at home, and you can get a higher temperature of the water by containing and the materials, the boiling steam in here under pressure, the higher, the pressure, the higher, the temperature at which water boils and this reactor, which is called a high pressure pressure reactor.

The goal is to have very high pressures and also very high temperatures of the water, but not to have the water boil. The water is kept from boiling by the high pressure inside. If the por V opens the pressure falls and eventually you can have water boil what happened in this particular case? If he came a big problem, was that the pur Val Pew RV was open and it was letting.

Steam initially steam and then water from the reactor to be pumped out above it's almost as if you had a boiling over pot on the stove with a tight lid on it. Eventually you'll get steam coming out at first, but eventually water will start to come out. And in fact, the coolant from the reactor was pushed up by boiling of the contents here, because it was so hot.

Push the coolant up through the pressurizer, through the PR RV and into a tank that was nearby. And then when this tank got too full, it overflowed into the bottom of the reactor building itself. The consequence was that the water level in the reactor fell and the core became uncovered. When you uncover the core, you run into all sorts of problems.

Now there is no way to get the heat out. And because you can't get the heat from this, there's no way.

Oh no, that wasn't no, don't we just lost the others, so we don't hang up.

Okay. Okay, go ahead. Sorry. As the water level fell, the reactor core became uncovered. And when the reactor core became uncovered the temperature in the fuel elements, Rose, and they melted. And when they melted, because they're close together and they started to melt, they formed a kind of clump. And now there's no place for the water to pass through in between these, it makes it solid.

It's almost you've got a whole stack of little columns, but you melt stuff on the top and it makes a solid clump. And so there were lots of places inside the reactor now where water couldn't circulate, but the temperature was very high and steam formed. Okay. And these were steep bits of steam that were trapped inside the reactor core, not able to escape.

And as a consequence, the temperature went even higher here and it melted more of the the circle stuff. And you got this kind of reaction of essentially having the core start to slump. One of the problems with this thing, the circle is if you get the temperature high enough, you go from having the to water and you produce from this.

Hydrogen and zirconium oxide. And the consequence of this is that there started to be hydrogen generated by this process as a by-product of the high heat and the exposure of the circle Lloyd and the hydrogen itself becomes a problem because it is not only something that can come through here, but it trapped in here.

If you start to combine it with oxygen, you can have an explosion. And there was a lot of concern about this explosion. What was interesting. Was from a control room standpoint. And you can see this in some of the diagrams.

Is that in the control room? Hold on. Let's see if we can get Sandy going here again. Try that again. If you look at the control room, there's nothing in the control room. That says here's the reactor vessel. And here is the water level. There's nothing like that in the control room. You can't look at any, you can't look at any kind of meter and say, Oh, this is what the level of the water and the reactor is.

All of this comes about your understanding of this comes about from a process of inference that you have to make by looking at many different knobs and dials, the settings of all sorts of switches, various kinds of indicators,

series of meters and valves, but nothing that is

shouting.

I can hear somebody. I can hear somebody's phone ringing. I'm here. I can hear you. Okay.

So what has to happen is that the operator

has to look at these collections of data.

And from this, make an inference about what the water level is because there is no, because there is no direct measure here. All of this is inferential, and it's usually done by looking at the temperatures and the pressures of various valves that measure things at different locations. The problem here was that the reactors behavior, the nuclear power plants behavior didn't make any sense.

It didn't make sense. Because, although it appeared that the system was working and it, and there was an indicator light that was on that says the PO PRV is closed. It says the PRV is closed. This is just an indicator for a solenoid that is supposed to close the valve, not its actual location. So there's a.

Circuit there, which is supposed to activate a solenoid, which is supposed to allow the valve to close and the solenoid can act, but the valve may be stuck open. But the only indicator that they had was the, this light, which measures the electricity, going through the soul. And I not be actual position of the valve.

There was some indication that the por V was open, but it was actually on. Panels on the, on a panel that was behind the main display. So a person standing out here looking at the main display cannot see these indicators. But if you look at these indicators, what you would see is that the temperature is, and the space beyond in the piping, beyond the pur V was actually very high, which means that hot water, hot steam is going through that.

That is there are, there is information in the plant while the accident is happening, that someone could have looked at and use to make a diagnosis about why the plant was in the state, that it was

trying to these guys who run these things are all nuclear engineers from the U S Navy. So they're very used to following procedures and doing things by the book. They're very used to using their specific trained routines to do things. They've got a lots and lots of training. They're this close to an elite group, as you can have.

And the training that these guys had

told them, that was one of the things that they were concerned about is if the water, the reactor goes, gets full of water. And so does the pressurizer above it? If these things are both filled with water, the system has done something called go solid. It's gone solid. Usually there's a space in the pressurizer where there is a bunch of steam up above the water and the steam and the water kind of equilibrium.

As the temperature goes up, there's more pressure from the steam. It raises the pressure and things enforces the boiling, the water, not to boil. And as the temperature goes down, the size of the steam will, will. The amount of water in here will shrink. And the size of the steam bubble increase the steam acts as a kind of the steam bubble acts as a kind of resiliency, buffer a space in there that can be expandable in and out to allow this system to work.

But if you ever fill this up, You have nothing. You now have no, no buffer in there. And the thing begins to react very violently. The problem of course, was that with this, you should have very high pressure

and they were very concerned about this condition. But in fact, the pressures that were being measured were actually quite low and they struggled for a long time hours to figure out how they could have low pressures. Being afraid of going shallot apparently have the por V valve closed.

And in fact, inferred that the reactor had lots of water in it. That the problem was not that they had too little water, but that they had too much. And as a consequence, they turned off the automatic pumps whose function it is. To drive water into the reactor. There's an emergency system that is automatically supposed to turn on the pumps and drive water into the reactor.

And that system actually triggered, but they turned it off because they believed that the reactor was in fact too full, rather than to empty. This also was a problem because these pumps had been known to trigger automatically without having anything going on. So the pumps were thought to be triggering in this kind of sporadic way, rather than for a good reason.

It's clear in retrospect that if the operators had done absolutely nothing, if they had not touched any of the controls of the power plant after the turbine tripped that the plant would have shut itself down safe. It would the high pressure injection of water and all these different sources would have kept the core covered.

It would have had flooding. They would have had some stuff go into the reactor containment building. There was all sorts of problems that would have occurred, but there would have been no great catastrophe. The reactor would have shut itself down essentially. All the backup safety systems would have kicked in.

They would have provided the coverage that was needed and that the accident would have essentially not happened in the sense that the reactor core wouldn't have been uncovered and therefore melted. Once you melt the reactor core, the plant is a total loss,

so they could have conceivably had a plant that was functional and working at the end of this, if they only hadn't touched the controls. And so after three mile Island

in 1979, after three mile Island, the big question was, why did the operators?

No, I don't understand.

Why didn't they see what was going on with the plant? What was the problem with their mental processes that led them to believe that something was happening in the plant other than a loss of cooling accident? Now, this question was answered by saying this is a case of human error. If you read the report, it is.

Particularly Pigford's commentaries, what he says is, look, the plant worked fine. The systems work fine. No radiation got released. It wasn't really a problem. It would have been great if people hadn't screwed it up, basically he says, look, you've got these operators. They aren't very well-trained. They don't know what's going on. They jump to conclusions. They mishandle the plant and they drive it into the ground. It's impossible to make systems that are safe in the sense that no one can drive them into the ground. The automated system worked fine, but you have these operators performing something that wasn't necessary.

And in fact, made the plant, they drove the plant into a disaster. It didn't happen on its own. It happened because they were willing to work with it. With this and play with it in a way that they shouldn't have, they should have just sat back with their arms, folded, let the automated systems take care of it and everything would have been fine.

This is a model. And that's what Pigford says. This is basically if you're a, if you're a plant operator, you should just sit back and not touch any of the knobs or dials. Let everything happen. It's built to be safe. It will be safe. There are a couple of problems with this particular view of describing this as operator error and the problems come up.

When you realize that it took months and months to figure out what had actually happened. That is although it's clear that the data was present at the time that might have led you to conclude what had happened. Nobody really knew what had happened immediately afterwards. It took them a really long time to figure it out.

They had to go through lots and lots of traces, lots and lots of data. In fact, the only reason that they were able to figure it out was because during the accident, in the control room, there were a set of recorders that had been installed. By the manufacturer of the plant of the reactor Babcock Wilcox, which were there just to record its operation during startup.

And if we're getting going, this is a new plant and they were just taking this extra data to see if they could see that everything was working right without this set of recordings that were made, probably no one would be able to figure out what had actually happened to the plant, by the way, none of those recordings were available to the operators at the time.

It was a black box away somewhere. So they didn't have that. They had to figure out they would have had to go around and look at these sorts of things. Hidden displays and indicators understand that some of the indications that they were getting were in fact incorrect in the sense that they indicated something that was not true.

And they would have had to put that all together in order to be able to form a correct view of what was happening in the power plant inside the reactor and take the right sorts of actions, which would have been to let the automated systems continue. I need to pour water in there and keep the reactor cool.

This is a really big event. It's the biggest event in nuclear history in the United States. It causes the end of the nuclear industry in the United States. Although some people will argue that's not the case. In fact, it basically destroyed the industry. It, after that, we didn't have any new plants commissioned and designed in the U S and nuclear power became essentially a dead object.

All the activities related to developing new power plants. Went to France and to Japan. But there were no more activities in the U S it ended up being a dead end. It was also such a big event that it really wasn't possible to just say, Oh, it was a case of human error and therefore we're done, if somebody drives a car the wrong way on the freeway, if somebody breaks a a component of an airplane.

If somebody crashes a plane, these are all events, which are small enough that people can say, Oh, that's a case of human error. And that's a reasonably satisfactory explanation. But in the case of three mile Island, it wasn't an adequate explanation. It was too big and event. And the plant had been too carefully designed and the people in fact were not a bunch of.

Sort of low level, not very smart people. They were the best at the U S Navy had to offer these people were all former submarine, nuclear reactor engineers who had years and years of experience and lots of training there, all the arguments that these people needed to be backed up by stronger people, or they needed to have more training or all those other things were they didn't th those explanations didn't sound very good.

They didn't explain how it was that these people could have been confused about what was actually happening in the plant. This turns out to be the watershed from here everything starts right here in March of 1979.

All of what we now call safety in complex systems, all of what we now do in cognitive work studies is derived from this event. And the reason is largely because this idea that human error was **the cause **became itself, something that needed to be studied and it generated a whole series of new researches and new quests and new examinations of the topic of error itself. The question would became, what does it mean to make an error in this setting? What is an error? These guys actually believe that they were doing the right things with the plant, but that in fact they had a model of what the plant was doing that was incorrect. They had the wrong model.

In fact, there were multiple different kinds of models that there that one could have had about what's happening in the plant. But the one that they should have had that everybody understands, a significant loss of cooling accident, was not the one that they had in their minds.

They didn't get this, they got something else. And the thing that perturbed people and made them very upset was there, were there seem to be multiple opportunities for people to have gotten the right diagnosis. There were lots of opportunities during this few hours at the beginning of the day, starting at 4:00 AM where people could have under other circumstances realized that they were having a loss of cooling accident.

In fact, nobody did, they all became focused or fixated on a different kind of an accident. Some sort of weird accident with essentially two much water in the core, too much water was present. This is what they were thinking. And in fact, it's not just one operator, but many of the operators had this view that the world was a certain way.

When in fact it was another way. And this later on became, came to be called a fixation error.

We now understand a little more about this but what happened in three mile Island in those first few hours is everybody became fixated on one collection of data and one particular set of hypotheses about how the plant was behaving. And they could not, in some sense, see, we're not talking about seeing the visual sense, but see, in the sense of understanding, they could not see that this was a better fit for what they were examining in terms of data.

Than this. This is a pretty profound thing because it turns out that there are in the evolution of the accident. There are people who come to the scene and immediately recognize problems. One of the first things that gets recognized is that there's another valve out here called a block valve, which was in fact open.

And they you're supposed to close the block valve when you have this kind of emergency. And they went and closed the block valve, which stopped the flow out of this. And another one realized that in fact, they needed high pressure water and turned on the high pressure water, but it was turned off shortly afterwards.

But closing this block valve may have in fact kept much of the damage to the core from actually occurring. Because once you closed that, now the system became closed again. And it was possible to. To look for cooling, much of their energy was spent trying to start up. These various pumps, all the pumps had been closed down, had been turned off and now they, the pumps and the lines themselves were filled with bubbles and the bubbles things from getting started, creating an airlock like you can sometimes have, if you've ever had this problem, trying to get the water flowing in your kitchen.

Sometimes you'll have an air lock that doesn't let that water flow. And they had airlocks and a number of these things. We couldn't find a way to get the. The energy out of the plant, the new research and new examinations came in a variety of different ways.

But the key things for us is that the people who were involved are the ones who are now thought of as the fathers of the field that we're working in. So in Denmark, you had Rasmussen and Lind. In the U.S. you had Woods and people like John Senders, Neville Moray. I'll say other countries other than Denmark. And in England you had folks like James reason. And again, in the U S you had people from other areas like sociology, like Charles Perrow. Yeah. But all of these people started looking closely at the issue of error as a cause and understanding how people made these kinds of diagnosis and could be wrong about them, as a result of Three Mile Island.

It's not before that it all happens now. And in fact there is a number of, there are a number of pieces of paper that comes out of a number of reports, some things, but they're in this kind of report for them. Like the thing that you saw, which is that, that it's an internal report designed for internal circulation.

It's not something that's published in a journal. That's why I included this one from 1981, the resell report, because these guys are all talking with each other and writing things back and forth and having various discussions about. What's really going on here. What's important. How is it that people can sometimes have this?

Not this it's pretty clear that this is not one of those human factors, events like lowering the landing gear. This isn't a matter of getting the knobs or the dials, correct. This is a problem with high level understandings about what the plant is doing and what should be happening. It's at a, it's at a much, much bigger level.

And it's from this. That all of the research that we now have on safety is essentially drive anything. There, there is basically in the world of safety, pre TMI, and post TMI, but TMI is the critical event. It's where everything comes together because the event is so large and so difficult to explain if the plant had failed in some mechanic ethical way.

If there had been a rupture of a true loss of cooling action, big, some pipe burst or something like that. It would be quite different. Now the fact that the plant failed in a way that required the active participation of people is much more troubling than that. The plant failed in some sort of mechanical way, because now it asks you the question.

Why do we have operators in these things? What are operators there for? If what operators can do is essentially make the plant crash, what do we need the operators for? And it turns out that an exploration of this is what starts to take place starting in 1980 and 1981. And you get Rasmussen's report, the one that you saw, which comes out. And there's a version, there's an early collection of stuff in 1979. And then 1981, he comes out with the report and the report is titled &quot;_Coping with Complexity_&quot;. It's not error that he's focused on. He's focused on complexity because what Rasmussen, who is an engineer and has looked at these engineering kinds of things for a long time, realizes is that these guys have not so much made an error as the complexity of the world has defeated their abilities to understand what's going on. And he tries to understand how it is that people who are in this complex world go about this process of making a set of inferences about what the world is doing.

And in particular, in sophisticated control worlds where the only thing that the operator has to look at, are the sets of panels that are in the control room, that is, you can't look at the reactor and see it, you're looking at indicators. In this kind of world, he begins to ask the question: What should this collection of information look like, in order that these people, when they're making inferences about this come to the right conclusion? That's really the question that he's trying to answer.

And that's where skills, rules, and knowledge comes from. Skills, rules, and knowledge is a very specific focused paper, the goal of which is to direct attention at this particular problem, which is that the layout of information in the plant and the presentation of the information to the operators is in fact perfect if the plant is running perfectly. And completely useless, and in fact misleading, if the plant begins to have a major transient. All the alarms are going off. In fact, the alarm that says that there's water, there's fluid flowing in this and that they're going to overflow the tank for this, goes off. But because the audible alarm has one sound for all alarms and you reset that alarm sound by pressing the reset button, it's just one of hundreds of audible alarms that are triggering during that period of time. So they don't really realize what they're resetting. They're just trying to get rid of all the noise because there's so many lights flashing and so many things blinking. And in fact this is still hidden from them.

They don't actually see, that is, understand what's going on until later on in the day. By noon of the next day, they have understood what's happened, although they don't really communicate it very well, but they've understood that they'd had a core problem and the plant is essentially lost after that.

Although there's lots of other stuff about hydrogen bubbles and people being evacuated and stuff. Those are all sort of secondary kinds of features. They're not critical to our understanding here.

This fixation idea, I think is a really important one because it's the fixation that becomes the real problem in understanding. It's not the case that one person was badly trained didn't understand this, he was there all by himself, working at night and somehow screwed up. Everybody who's there is caught in the same wrong framework and therefore it's a much more robust phenomenon that's happening. It's not just because of one person.

By the way , in the report there on page nine there's a discussion there about the Navy technicians and they're working and he says , _There is always a sense of urgency to a Navy technician's watch, a sense that this reactor is powering a ship of war on a mission, that he is accountable for his part, however small a fragment, of the total operation, and that if he is stumped, his superiors are close at hand, and they know the equipment better than he does._

This is pretty much the beginning of what we will later call safety culture. The distinction between the world in which the nuclear reactor operators in the Navy work and the world of large power generation is one between the presence and the absence of what we think of as a safety culture.

These are people who brought their skills from the Navy, but they didn't enter a world of culture. They entered a world of bureaucracy and regulations or people blaming each other for various things and failing to do work on safety and so forth. And so they're stuck. They're outside of their world. And for this particular transient, they're at a great disadvantage.

The formal response to Three Mile Island is the old response, which is the human factors sort of basic watchword, which is blame and train. If you look at what the reports say, they want to identify these operators as the failure. They are, after all ,the ones made the errors, and their idea is more training will somehow fix their problem. That if we train them better, they'll become better at dealing with this and so forth. But nobody really buys this. Because these guys are already well-trained and in fact, there's plenty of technical stuff there. It's much more sophisticated than that.

And so the training description doesn't actually cut it. It's not sufficient to describe what has actually happened than to correct it.

It's Rasmussen's 1981 paper on complexity, where he says for industrial plants, the complexity faced by operators is determined by the representation of the internal state of the system, which the interface allows the operator to develop for various work conditions. This is Rasmussen saying, it doesn't matter what's happening here.

You can't have this without doing some inference from this collection of data that you see here. And the question then becomes, it really becomes how do you design the control room so that people will be able to figure out what's going on? And the problem of course, is that you don't have to design the control room just so that it works when the plant is working fine. You have to think about all the various ways in which the plant can fail and allow the operators to be able to figure things out in those conditions. And it turns out that the plants can fail in ways that nobody really expected.

This particular failure, that is not a straight loss of cooling accident, where you cut across a three-foot pipe and all the water's draining out, but one where it's floating out through this relief valve that isn't really thought to be a major part of the system. This process here is not one that they thought of. In fact, they had the same accident at another nuclear power plant earlier the previous year and where this particular valve, it's stuck open, they got very much the same behavior, but they recognized it and corrected it. This is not an unheard of failure, but a failure for which no one had really done anything or considered anything about how to deal with these particular problems.

In skills, rules, and knowledge, Rasmussen finally takes a hold of this and develops a kind of model or system that he thinks of that's going to be able to help understand why plants fail and how you build interfaces, what you need to do. Skills rules and knowledge is one of these papers that gets cited all the time, but is rarely read and skills, rules, and knowledge as a topic is one of the things that is most often misunderstood by people who are trying to figure out how to make human performance better.

What Rasmussen says is that there are signs in the world. There are signals and there are symbols and that human processing of data can sometimes take place down at the signal level, especially if it's a fine motor activity like controlling a knob, an example would be bicycling. Or trying to play a video game in which you have to some simple task like pong, where you're trying to move something back and forth. Anything that involves fine analog motor movements.

In that world, people can work directly on signals, concrete things coming from the world, their actual sensory impressions allow them to do this. That is, you can look at sensory input processing in the brain and find direct connections to the motor activities that people are having. And there's not much else that's required.

But for all other kinds of activities, there must be some interpretation of these things that are going on in the world so that people can understand what they mean. That is the signals become signs that have some particular meaning. The meaning isn't carried in the signal itself, it's carried in an understanding of the world that the person has that they've imposed on it. The meaning of a sign is derived from the understanding of the world. It's teleological rather than simply derived from the signal itself.

The example that Rasmussen gives is, you can have the same meter mean three different things. This can be a signal if what you are doing is trying to, for instance, use a control knob and turn to get the meter to read something in a short period of time. That's just a signal. The interaction is a motor thing between how you turn the knob and what the needle does.

Another use would be as a sign, which is where this is actually measuring the pressure. And you understand that this is a representation of pressure, not a needle pointing in a particular direction. The only way you can know this is by some outside thing. There's nothing about the needle and its location that makes you understand that it's a pressure (gauge) . You need to understand that meter has that value. You turn it into a sign. It's the same physical device, but its meaning is different.

And for these kinds of activities related to this, there can be a whole series of rules that lead to various kinds of activities that end up looking like motor stuff, turning knobs, controlling various things.

The biggest mistake that Rasmussen ever made, perhaps the only one ,from my view, is that he called this &quot;rules&quot;. Because it encouraged people to imagine that the rules he's talking about are the rules we write down. That is, they think of this rules in this cognitive model as being rules, only turn on X when Y and Z are above... and so on, and so forth... a rule, a synthetic rule that we've made that we've written down, like a rule that you might read in an operating manual or a rule that you might read in, in a cookbook or something like that. He's not talking about rules, external to the people he's talking about rules that people have cognitively, that they can use as an immediate way to react to things.

For example, in his example, he gives a place where you have a low and a high value of pressure. And the rule that you have is so long as the needle is between these two things, there's nothing wrong with that, and I don't need to do anything. But if it exceeds, if it goes past this on either side, then I must do something. And you can look at this and you might even make an adjustment to something without doing anything more than thinking about this needle and its location relative to the marks.

You have to work at this a little bit, but if you want look at his example, you'll be able to see it. He draws this out a little bit more in the paper. And finally, there's the level of the symbols, which is where the same meter means something different, which now has to do with an understanding of how this represents something in a system. For example, the pressure in the pressurizer, as a measurement here. And that the issue here is that we're thinking about this as the pressure in this particular location. That is it's become for us no longer, just a symbol of a sign of something, but actually becomes a symbol which represents some conditions someplace else that's part of our larger mental model.

And this then refers to different levels of activity. One is skill. The other is rules. And this third one is knowledge. And by knowledge processing, what Rasmussen means is symbolic processing using symbols and bringing them together to create some larger model.

This (skill) is very fast and it can become very skilled. You can do this, you can learn how to do this, and it doesn't require much conscious effort. You can ride a bicycle and listen to the radio at the same time.

This (rules) requires a little bit more effort, but you've automatically got the necessary sort of stuff. Again, it's not too effortful.

But doing this kind of symbolic processing, (knowledge) this process of actually applying symbols and understanding what they mean is itself quite effortful. It takes lots of time. It requires consideration. And, indeed, it requires a whole collection of knowledge, not just about what the symbols mean, but of how they're related and a lot of information about the goals. That is the only way symbols make sense to you is if you are able to understand some of what Rasmussen admits is the teleology of the system.

Teleology is a tough concept. It's a concept related to what matters and why. It's the why of what matters. Why does this matter? It matters because, and then you have to have some higher level goal there. And what Rasmussen argues is that human beings are in fact, teleologic systems. That to think about them as simply mechanical devices that respond in a reflex fashion misunderstands what they're doing because human beings are busy figuring out what is the right thing to do, and deciding on the basis of teleologic principles, rather than just simply following rules or applying a skill.

The operators in the plant had lots of these things (skills) in lots of these things (rules) . But when they got involved in this knowledge, symbolic level, the data that they were working with could not be manipulated for them to understand and formulate a correct model of the plant.

All the information that was flowing from the world was coming in as signals. But it didn't somehow get up into this knowledge world in the right sort of way. So the right symbols aren't being generated. And people are not therefore able to build a working mental model of what's happening in the plant.

Rasmussen talks here about mental models. It's one of the first, early times that mental models are developed, although Rouse is one of the first people to do that in his work. And there's a real fascination around this time and for several years afterwards, with mental models, which means the models that we have in our head of some portion of the world and how it works. And there's a tremendous amount of exploration that flows from this that is related to trying to discover those models, understand where their flaws are, understand how people get them. Because it's clear that people have them and can even run the models as a kind of mental simulation that allows them to figure out what's going on.

They also can do it in a kind of more mechanical way. They can say, _if_ the pressure here is _this_ and the temperature here is _that_, then _this_ must be happening. That kind of reasoning is symbolic reasoning. But again, it requires some sort of mental model in order for that to happen. And what Rasmussen is saying is what we really want to be able to do is to make sure that when people are processing all this stuff that we are supporting in them, the development of a mental model that corresponds with what's really going on in the world rather than something else.

If your mental model is in fact incorrect, if you've got the wrong mental model, then the actions that you're going to take are not going to be effective to the extent that the model doesn't correspond with reality.

So for Rasmussen, the problem becomes: of all the many different mental models... and there could be very many of these... how do we assure that when people are processing signals, that they're not that the symbols that they end up using end up leading them to, in fact, what is the correct mental model so they can actually control the plan. This, Tom's the sort of centerpiece of the next 10 years of work for from 1979 to 19, about 1990, almost all of the focus on understanding how to keep things like three mile Island from happening is really.

Intimately involved in this kind of model Rasmussen's paper in 83 early versions of it were coming out in 81. Many of the things in the paper, actually you can see in the coping with complexity paper, all of this stuff is a way of trying to get a handle on what's going on.

Is trying to get a handle on how people do this process so that they can build instrument control panels and sets of information and training and other things that will allow them to get to that right. Mental model, this high level symbolic representation of what is going on. Starting in about 1990, there's a whole nother way that takes off from this.

But what is that the people who are working around Rasmussen and in that group, sometimes the including people on the fringes, but basically mostly people who are coming from the nuclear world, that group of people keeps working on this and gives rise to a whole nother set of people.

For instance, Carl White and his sense-making

Is a direct outflow of this experience. The fact that operators are trying to make sense of what is going on and failing in it is the fundamental. Trigger point for all of this stuff that follows afterwards, the management people, the sociologists, the business school people, they're all basically concerned with, how do we build plants?

How do we build environments that allow us to manage these kinds of risky, potentially very hazardous systems by ensuring that operators actually get them right. Understanding of what's going on when they're faced with some kind of breakdown or flaw. What Rasmussen does in skills, rules, and knowledge is set the sort of basic philosophical and engineering requirements for that world.

But it's all about developing the right kinds of stuff, as he says,

he says in the present man-machine context, it seems to be important to keep the role of information as time-space signals, which are processed directly in a dynamic control of the motor performance, separate from the role as signs, which serve to modify actions at a higher level. But he wants people to do is to stop pretending that the kind of basic ordinary human factors from the 1940s is what's matters now because it's not just simply making all the numbers legible or making the controls easy to turn or giving them the right sort of layout.

It is actually building this process so that people can get the right mental model.

so Rasmuson then sets out finally, the agenda for this. He says, okay, the situation leads to the need for human performance analyses in real life situations to identify mental strategies and subjective performance criteria, he wants to start a revolution of looking into how people. Process information to understand what's going on in their world and use that information to create mental models, which they then try to use to manipulate it the world.

And he says from the analysis of task performance, by observation interviews, verbal protocols, error reports, et cetera, leading to descriptions of actual performance in a number of situations, generalizations across instances can lead to descriptions of prototypical performances. From which a repertoire formal strategies can be identified and described.

He's talking about uncovering how it is that people are doing this sort of stuff through a whole series of different kinds of research activities that then will go on to inform the development of the support mechanisms that are necessary to do this. And that is why we owe it all to three mile Island, nothing before three mile Island had the effect of doing this.

And the world changed by 1990. No one was talking about error in this sort of simple way. He made an error. Therefore the world failed. Everyone is trying to understand how it is that people can often get the right understanding and be able to do the right things. But sometimes are unable to do that.

You'd get fixated can get into the wrong mental model. Can miss cues and clues and that revolution starting in 1990 is what we'll talk about the next time. So questions

are you still there?

I can hear you.

Can you hear me? Yes, it's Christina.

I'm trying to get so I can see you. Yes, I lost the, there you are questions.

No one. I have a reflection after reading this. That's and none of this that we have at dumps far refers to design systems that are built by engineers. You actually have sort of a blueprint of a system. You have physical you have actually sort of natural science to guide your text via the system.

Whereas healthcare that I'm interested in It's uncharted territory. It's not designed to challenge them in the same way. Yeah. Do you have any comments on that? Because as I was reading this, I was thinking that I think that the methods are, completely, you can use them, but it would be exploring the system at the same time as you're exploring the people's perception of the system.

So I think that's absolutely true. The kind of system that you're talking about here is a carefully engineered, carefully designed system that has very specific characteristics. What's problematic about this system is it's transcended the barrier of complexity. That is, it's not just a complicated system.

It is a complex system. This means that its failure modes can not all be known. Not all of the circumstances can be planned for prior to TMI. All of the emphasis was on building up the set of procedures that operators should follow. They literally had racks filled with documents about what they should do.

If this happens here, you go to the page for this happens here and you look and you do these things. And they had spent enormous amounts of time trying to proceduralize that world so that the world would be safe. But in fact, it turned out that the procedures didn't work. There were situations that happened with, there were no procedures for this.

And it's ironic that we now live in a time in healthcare where just the opposite is true. We're trying to go from where we don't have any procedures to proceduralizing everything in getting somehow imagining that we can do enough of this. We'll be able to take a complex world and make it into justice.

Complicated one. But you're right. This is this is comparatively speaking, a relatively straightforward example, even though it's complex compared to the kinds of things. Extraordinary worlds of that you see in healthcare maybe not human beings are very complicated, complex but the number of external factors that you can rely on is actually greater in the care of human beings, because you can actually be there and see them.

You're going to look at essentially look at the reactor and the containment and see it in front of you. Although now there are situations that really look quite a bit like this. In the middle of a neurosurgical procedure when the patient is completely draped and covered up. And there are only the sensors on the outside to tell you what's going on.

Yeah. It begins to look a lot like a power plant, but if you're seeing somebody in the emergency room and the kid is screaming and pulling at his ear, you've got lots of important information that you as a clinician can use to figure out what's going on. However two things come from this one is that the techniques that, that Rasmussen is talking about that get developed in the late 1980s and early 1990s still are useful here.

The ways of doing these things, the kinds of verbal protocol analysis, observational studies, looking at what makes problems hard. And so on that stuff in these other areas, I would, one of the things that has happened is that Rasmussen has been very criticized. For his abstraction hierarchy. This idea that as you go up that there's, there are means, which are in fact for for there, for each level of an abstraction hierarchy.

There are things below, which are the means by which the school is accomplished. And there are things above it, which are in fact, the goals, the treat this as means. So everything that you think about in the abstraction hierarchy is both a goal and a means to something depending upon whether or not you're viewing it from above or from below.

And Rasmussen put very specific words on these levels of the abstraction hierarchy. He's got, he's got these five levels and he's got, he talks about the concrete physical device. And then he talks about its basic function. Then he talks about its complex function and it works its way up.

That clearly is very specific to nuclear power plants. It cannot be applied generally, the words that he's using, but the idea of an abstraction hierarchy, that there are goals. Some goals are more important than others is really true in all of these worlds. So we do have situations where we are unsure about whether the core is uncovered.

People are saying, can you feel a pulse? Okay. That's equivalent to saying is the Coran covered. And if you can't feel a pulse, you have to do something completely different that doing something completely different involves asking a higher going up a higher level of traction saying I don't care what it is that we're doing down here.

It turns out that this is, this goal is being threatened life, and we're going to start to do these other things. So we have the, you can map the abstraction hierarchies in all these different domains. If the problem is in human settings and particularly in healthcare. There is no stable system like this that you can talk about.

You can't go and do a diagram of a hospital operating room, which will show you all these different components. The nuclear plant is actually a very nice example to work these details out in, but actually doing the things that you're talking about requires a much we're flexible approach to the analysis.

So I think you're right, but at the same time, I think that the, that. In particular, the fact that we understand that just simply giving people better data, doesn't always solve our problems in a kind of narrow sense. We now recognize that people have to have this knowledge and understanding a symbolic understanding to be able to formulate the right mental model of the patient or the aircraft or the military campaign or the submarine scores or all these other things.

Another reflection that I had though, was that since we do like the knowledge level, that is maybe why we feel so systematically in trying to find the signals and indicators, to be able to look at our systems on a sort of macro level or less on levels for managers and for politicians and others, because I can see how we can actually be inside the nuclear court and do observation.

When we go down to the sort of level where the nuclear reaction actually happens in healthcare, the face to face person to pay the person that will escalate you move up in the system. It becomes less and less obvious how it works and what you should, even one of the things that's difficult about Rasmussen's hierarchy is that people think that it means different levels of organization.

Rather than different goals. And I'm not sure that's true. I don't think it's the case. The low levels represent the individual unit and the next level is the hospital. And the level above that is society. I think rather that this is the kinds of goals that are involved. The low-level ones being, I'd like the blood pressure to be normal.

Another one above that, meaning I need to have cardiac output and make sure that the heart is still beating and above that. I want to make sure that the life of this person. Is actually a satisfactory life. And I'm trying to preserve that and above that are things like whether or not it's useful to preserve life in this setting.

I think part of the reason that healthcare works is because unlike nuclear power plants, where people had to put together a few billion dollars worth of components, precisely to make it work. Healthcare is actually a whole bunch of small systems each running on its own so that the dynamics of the system are relatively straight forward inside of that small confined.

And they don't bleed over very much from one to the other. In fact, when they start to bleed over into each other, that's when many of the problems start to happen and you see this in the hospital. It's the reason that paper that I wrote with Rasmussen is called going solid. The whole point of this is that you can have situations that are similar to, to this where the system becomes uncontrollable, but in the main we've been successful because we've been able to keep the scale of activities required to care for patients relatively small so that the systems are relatively manageable.

They're still complex, but they're much smaller and unitized in such a way that we don't get failure across the system. If one component starts to go out. You can have a field operation and the patient may die, but that's not going to have an immediate cascade effect. That's going to rush through the entire system and shutting things down and causing the rest of the system to fail.

It turned out that in the nuclear power plant, they had exactly that kind of condition, although nobody really appreciated it. Even after Davis Bessey. No one really thought that you could ever end up in a situation where people wouldn't recognize that the PRV was stuck open. This is, and that's one of the things that was really difficult in the sequences in TMI, near the end of the accident, people are saying.

How can we have missed this? How could we not recognize it? The por view was stuck open. What's going, how can we not know? They themselves understood that they had gotten it wrong. And that's one of the key elements of fixation, by the way, if you think you're looking at a case of fixation, one of the most important criteria is that when people are actually then shown the correct diagnosis, they immediately agree.

If it, if that's not the case, then there's some knowledge deficit in those people. They don't understand how things work. But if you have somebody who is fixated on one thing and then you show them, what's correct. And they go up like this, you've got a case of fixation. That's one of the cues.

But I, I think that in this particular instance, you can see these things so clearly. And remember, this is now 35 years ago almost. We've got 35 years worth of work on these problems since this time. We've gone back to is the very beginnings of this appointment, which none of this stuff existed.

There was nothing, this, all this stuff really didn't exist. No one, I thought that it was important for the plant operators to have that understanding the belief was at the plant operators, follow the procedures in the book. Everything will be fine. But it turned out that the world could behave badly enough, that it could give the plant operators something where they couldn't find a page in the book that described what to do in that situation.

And indeed it, later on, when you look back on it, you realize that the function of the plant operators in a nuclear power plant is to turn off the alarms. There are so many alarms, there are so many things that are going to fail. It's such a big complex system that it's always going to be feeling. And the point of the operators is essentially to keep the automated systems from turning the plant off at the first opportunity, because that's what it will do.

The plant is so complicated and so sophisticated that it will always have various kinds of transients in front of them. Interestingly enough, the new plants that are being designed now, the, so the Arriva, the and the The Chinese plants and the Russian plants are all very much simpler than TMI in terms of their control schemes.

They basically said, we can't make something that complicated and assure its safety. So all of them are now simpler in design. They have less things for operators to do. There are less things to be controlled because they realized that they could not actually assure that people would be able to figure out what was going on when they came into these kinds of situations.

It can feel into many different ways. Other thoughts I told him it's going to increase in the science practice should decrease because of this, that you get tired of looking at them as being a breaths, having just the coursing less appearing money when something has happened. You can, if you can make the world simpler, it's a great thing.

But unfortunately the world, it's hard to make the world truly simpler. It's easy to make simplistic representations of the world that is easy to make a control room that has an on off switch. But the problem with that is, is whether or not you're willing to have an on off switch, be the only control available to, to control that phenomenon.

After this happened, a lot of emphasis was put in trying to build displays sometimes called parameter displays that would allow people to see all the data at one time so that they would somehow be able to figure it out. None of those were predict particularly productive. They did not help people actually solve the problem.

It was not a matter of seeing the parameters. There were developments in trying to have people understand things like that balance of energy and mass in the system and drying grams and drawings that showed that as not as here's a valve and here's a pipe, but here's how much mass is in the system.

How much of it, how much of the energy is located in the system where that has been going, where the steam, where we are in the steam relationships between water and steam. And in fact, there were a whole series of programs. One of them called steamer. Which were developed to test and understand how operators could make diagnosis of this.

But the problem is that, that, and this is especially true in healthcare in order to accomplish the things that we wanted to accomplish. We ended up encountering very complicated situations. We could easily say, Oh, we're just going to have one drug. It's going to be aspirin. Okay, that's a great way.

And you can probably deliver it pretty safely. The problem is you can only, there are only a certain number of diseases you can treat with that. And so we end up in the situation where in order to accomplish our ends, we need more and more complex systems. And we're still struggling with trying to figure out how to manage those sorts of things. That struggle is still ongoing.

Everybody will cite _skills, rules, and knowledge_. At different times, you'll see it in a lot of citations. It's one of the most wonderful papers that's ever been written. From my reading it now it's actually quite a pleasure because you can see in it that Rasmussen here is foreshadowing a whole bunch of stuff that's going to come.

What he writes about in here is essentially the agenda for the next ten years. And people go on and pursue each one of these activities and they do all the stuff that he is talking about in an effort to get a handle on this. He puts it all together in this particular setting, he describes what the problem is. He describes the underlying theory. He describes the obstacles and the important things that need to be done. He sets out the research program and people go off and do it.

The _one_ thing that he got into trouble with is rules. Because when he used rules, people immediately thought, &quot;Ah... what I need to do is develop a good enough set of rules that has policies and procedures that nobody gets confused&quot;.

But he wasn't talking about rules in the world; he wasn't talking about rules in the sense of the volumes of paper that we read. He was talking about human cognition inside this figure (gestures to whiteboard) . And he's talking about the rules that are built into people's own processing.

Okay. Thank you very much.

